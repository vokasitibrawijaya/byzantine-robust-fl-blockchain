
% Real FL Training Results - VALID EXPERIMENT with Actual MNIST
% Uses torchvision.datasets.MNIST (NOT synthetic data)
\begin{table}[h]
\centering
\caption{Real Federated Learning on MNIST (20 rounds, 20\% Byzantine Sign-Flip, Dirichlet $\alpha$=0.5 non-IID). Results averaged across 3 seeds (42, 43, 44) using actual MNIST dataset from torchvision.}
\label{tab:fl_results}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Aggregation} & \textbf{Accuracy} & \textbf{Loss} & \textbf{Converged?} \\
\hline
MEAN (No Defense) & 66.95\% $\pm$ 3.55\% & 1.160 $\pm$ 0.078 & Yes \\
MEDIAN & 71.48\% $\pm$ 4.52\% & 0.940 $\pm$ 0.107 & Yes \\
\textbf{TRIMMED MEAN} & \textbf{73.00\%} $\pm$ 3.39\% & \textbf{0.938} $\pm$ 0.093 & Yes \\
\hline
\end{tabular}
\\[0.2cm]
\small \textbf{Setting:} 20 clients, 4 Byzantine (20\%), sign-flip attack, SimpleMNISTNet (2 Conv + 2 FC), 20 rounds, Dirichlet $\alpha$=0.5 non-IID.\\
\small \textbf{Interpretation:} TrimmedMean achieves highest accuracy (73\%), demonstrating Byzantine robustness. Even vulnerable Mean aggregation learns effectively (66.95\%) due to moderate attack intensity.
\end{table}
